{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from util import DampedHarmonicOscillatorDataset, simple_nn, create_tensors\n",
    "\n",
    "plt.style.use(['science', 'ieee'])\n",
    "torch.manual_seed(1024)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.device(device)\n",
    "print(f\"Training device: {device}\")\n",
    "\n",
    "# Model settings\n",
    "model = simple_nn(4, 1, 32, 8).to(device)\n",
    "supervised = True\n",
    "if supervised == True:\n",
    "    model_name = f\"supervised_pinn\"\n",
    "else:\n",
    "    model_name = f\"unsupervised_pinn\"\n",
    "num_epochs = 200_000\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "patience = 1000  # Number of epochs to wait for loss improvement\n",
    "loss_curve = {\"set\": {}, \"epoch\": {}, \"loss\": {}}\n",
    "\n",
    "# Parameters\n",
    "k_values = [100, 200, 300, 400, 500] # spring constant (N/m)\n",
    "c_values = [2, 4, 6, 8, 10] # damping constant (N.s/m)\n",
    "m_values = [0.5, 1.0, 1.5, 2.0, 2.5] # load mass (kg)\n",
    "t_range = (0.0, 1.0) # time domin (sec)\n",
    "num_samples = 1000\n",
    "num_train_sets = 5  # Number of training sets to generate\n",
    "\n",
    "# Create the dataset class instance\n",
    "dataset = DampedHarmonicOscillatorDataset(k_values, c_values, m_values, t_range, num_samples)\n",
    "# Generate training sets\n",
    "training_sets = dataset.generate_training_sets(num_train_sets)\n",
    "test_set = (500, 10, 2.5)\n",
    "test_dataset = dataset.generate_testing_set(*test_set)\n",
    "\n",
    "# Loss calculations\n",
    "def loss_function(model, criterion, features, labels):\n",
    "    # Data loss\n",
    "    if supervised == True:\n",
    "        pred = model(features)\n",
    "        loss1 = criterion(pred, labels)\n",
    "    \n",
    "    final_loss = loss1\n",
    "\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "# Model training \n",
    "def train_network(train_loader, model, num_epochs, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Define early stopping parameters\n",
    "    best_loss = float(\"inf\")\n",
    "    counter = 0\n",
    "\n",
    "    pbar = tqdm(total=num_epochs, desc=\"Training Progress\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(features)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            # loss = loss_function(model, criterion, features, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_curve['set'][set_number].append(set_number)\n",
    "            loss_curve['epoch'][set_number].append(epoch)\n",
    "            loss_curve['loss'][set_number].append(loss.item())\n",
    "\n",
    "        # Check for loss improvement\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        # Check if early stopping criteria are met\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch+1} due to lack of loss improvement.')\n",
    "            break\n",
    "\n",
    "        # Update the progress bar description\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 38/200000 [00:19<5:31:57, 10.04it/s, Loss=0.275]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Starting Training!\")\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    for set_number, set_data in training_sets.items():\n",
    "        features = set_data[['t', 'k', 'c', 'm']].values\n",
    "        labels = set_data[['x']].values\n",
    "\n",
    "        features_tensor, labels_tensor = create_tensors(features, labels, device)\n",
    "        dataset = TensorDataset(features_tensor, labels_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
    "        print(f\"Now training on: {set_number}\")\n",
    "\n",
    "        loss_curve['set'][set_number] = []\n",
    "        loss_curve['epoch'][set_number] = []\n",
    "        loss_curve['loss'][set_number] = []\n",
    "\n",
    "        train_network(train_loader, model, num_epochs, learning_rate)\n",
    "        \n",
    "    end_time = time.time()  # Stop the timer\n",
    "    training_time = end_time - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\\n\")\n",
    "    print(f\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Calculate the number of sets\n",
    "num_sets = len(loss_curve['set'])\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_rows = (num_sets + 1) // 2  # Ensures there's enough space for all sets\n",
    "num_cols = 2\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 6))\n",
    "fig.suptitle(\"Loss Curves for Different Sets\")\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through each set and plot its loss curve\n",
    "for i, set_number in enumerate(loss_curve['set']):\n",
    "    set_losses = loss_curve['loss'][set_number]\n",
    "    set_epochs = loss_curve['epoch'][set_number]\n",
    "\n",
    "    ax = axes[i]\n",
    "    ax.plot(set_epochs, set_losses, label=f\"{set_number}\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(f\"Set {set_number}\")\n",
    "    # ax.legend()\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for i in range(num_sets, num_rows * num_cols):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
